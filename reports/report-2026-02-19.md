# The Hidden Mechanics of How LLMs *Really* Work

Today’s arXiv drop is a masterclass in looking under the hood. Forget benchmarks and leaderboards—the most exciting papers aren’t about *what* large language models can do, but **how they do it, and how we can control the machinery**. The themes are clear: we’re moving from treating LLMs as monolithic text generators to understanding them as complex systems with internal gears, levers, and, sometimes, surprising blind spots. The story today is about **reasoning, representation, and reliability**.

Let’s start with a fundamental puzzle. Why can you aggressively compress a prompt asking for Python code and still get a great result, but if you compress a math problem the same way, the model falls apart? [The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts](https://arxiv.org/abs/2602.15843) cracks this wide open with a brilliant mechanistic insight. The standard method for compression—pruning tokens the model finds “predictable”—backfires spectacularly for math. It turns out that in a math problem, the most predictable tokens are often the **crucial numbers**. The model sees “Solve for x: 2x + 5 =” and thinks “13” is a very likely next token, so it gets pruned! For code, the high-perplexity, essential syntax tokens like `def` and `return` are correctly preserved. The authors don't just diagnose this; they build a smart, adaptive compressor that knows to stop squeezing before breaking the reasoning. This isn't just a neat trick—it’s a lesson in how a model’s internal probability landscape is intimately tied to task success.

This idea—that we can understand and manipulate internal representations—echoes through several papers. Take grammar. We know LLMs can be syntactically brittle. [Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs](https://arxiv.org/abs/2602.15846) offers a surgical fix: a bolt-on module that feeds pre-computed parse trees into a frozen model via a gated cross-attention branch. It’s like giving the model a dedicated grammar co-processor that doesn’t interfere with its existing knowledge. Similarly, [Surgical Activation Steering via Generative Causal Mediation](https://arxiv.org/abs/2602.16080) provides a scalpel, not a sledgehammer, for steering model behavior. By using contrastive examples to trace *causal pathways*, it can identify the handful of attention heads responsible for, say, a refusal behavior, and tweak only those. This precision is the future of model editing.

But can we trust the model’s own stated reasoning? The classic Chain-of-Thought (CoT) promises transparency, but there’s a nagging doubt: is the explanation faithful, or just a plausible story? [Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution](https://arxiv.org/abs/2602.16154) tackles this head-on with a clever social test. If a reasoning trace is truly faithful, then other “listener” models should be able to follow it to the same conclusion. By training a “speaker” model to satisfy a panel of diverse listeners, REMuL produces CoTs that are both more faithful *and* lead to better answers, dissolving the assumed trade-off. It turns reasoning into a collaborative, verifiable process.

This focus on process over product is a powerhouse theme. [Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning](https://arxiv.org/abs/2602.15863) makes a subtle but profound point. The benefit of having a model create its own example before solving a problem isn’t in the example text itself—it’s in the *act of creation*. When researchers decoupled the process (providing a pre-written example), the gains vanished. The model needs to *do the work* to prime its own reasoning circuits. It’s not about the destination; it’s about the journey.

So, what are the limits of this journey? [Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning](https://arxiv.org/abs/2602.15868) provides perhaps the most satisfyingly rigorous framework I’ve seen. It maps the entire transformer pipeline onto a deterministic multi-tape Turing machine. This formal lens explains why LLMs fail on “trivially computable” tasks like counting letters in a word: the tokenization tape (Tape 3) destroys the character-level structure needed for the task, and the standard pipeline has no subroutine to reconstruct it. It brilliantly reframes CoT as the model externalizing computation onto its output tape to work around these internal limitations. This is mechanistic interpretability with the clarity of theoretical computer science.

All this reasoning happens within a scaffold of **state representation**. [State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models](https://arxiv.org/abs/2602.15858) delivers a crucial message for anyone building LLM agents: your choice of how to represent the problem to the model (full history vs. summary, JSON vs. natural language, image vs. text map) is as important as the model itself. One stunning finding: for spatial reasoning, a textual grid is better than an image, not because of the grid itself, but because **forcing the model to construct the grid** elicits the step-by-step reasoning a pre-rendered image does not. The interface *elicits* the capability.

We’re also seeing a wave of innovation aimed at making these powerful mechanics **faster and more efficient**. [CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill](https://arxiv.org/abs/2602.16054) tackles the slow prefill stage by simply aggregating token importance scores across multiple layers, creating a stabler, better ranking to decide which prompt tokens to process. [Maniscope: Reranker Optimization via Geodesic Distances on k-NN Manifolds](https://arxiv.org/abs/2602.15860) makes retrieval-augmented generation (RAG) blazing fast by using graph algorithms on document embeddings to capture semantic closeness that flat cosine similarity misses, enabling sub-10ms reranking.

Perhaps the most mind-bending efficiency story is [Doc-to-LoRA: Learning to Instantly Internalize Contexts](https://arxiv.org/abs/2602.15902). Why repeatedly process a long document for every query? This hypernetwork meta-learns to read a document and, in a single forward pass, spit out a tiny LoRA adapter that “installs” that document’s knowledge directly into the model’s parameters. It’s context distillation at the speed of inference.

Which brings us to the grand, unifying view. [From Growing to Looping: A Unified View of Iterative Computation in LLMs](https://arxiv.org/abs/2602.16490) elegantly ties together two seemingly distinct training tricks—gradually growing model depth, and re-using layers in a loop—under one umbrella: **iterative computation**. They show these methods converge to similar internal signatures and, coolest of all, that you can train with depth-growing and then get an extra boost by applying inference-time looping. They are two sides of the same coin, teaching the model to iterate and refine.

Finally, we must remember these systems don’t exist in a vacuum. The haunting finding from [Evidence for Daily and Weekly Periodic Variability in GPT-4o Performance](https://arxiv.org/abs/2602.15889) is a wake-up call. Even with a fixed model snapshot and prompt, performance on a physics test fluctuates with a daily and weekly rhythm, likely tied to server load. It’s a circadian rhythm for AI, a stark reminder that our “deterministic” models are embedded in a variable physical world.

Today’s papers paint a picture of a field maturing. We are no longer just observers of emergent behavior; we are becoming engineers of internal mechanisms, cartographers of representation spaces, and theorists of computational limits. The goal is no longer just a model that works, but a system we can truly understand, control, and trust.